---
title: 'RATs: Input and Settings'
author: "Kimon Froussios"
date: "08 MAR 2018"
output:
  html_document:
    keep_md: yes
    theme: readable
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
vignette: >
  \usepackage[utf8]{inputenc}
  %\VignetteIndexEntry{RATs 2: Input & Settings}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

***

```{r, include=FALSE}
library(rats)
```

```{r, eval=FALSE}
library(rats)
```


***

# Input formats

## Data

RATs can work with several input types:

1. [Salmon](https://github.com/COMBINE-lab/salmon) and [Kallisto](http://pachterlab.github.io/kallisto/) quantifications.
2. Bootstrapped abundance estimates in lists of R `data.table`s.
3. Abundance estimates in R `data.table`s.

As of `v0.6.0`, input from a `sleuth` object is no longer supported by RATs. This is due to changes in Sleuth `v0.29` that made the
bootstrap data no longer available. Instead, RATs already supports input directly from the Kallisto/Salmon quantifications.

For option 1, the function `fish4rodents()` will load the data into tables suitable for option 2. Details in the respective 
section below.

For options 2 and 3, the format of the tables is as in the example below. The first column contains the transcript identifiers.
Subsequent columns contain the abundances.

```{r, echo=FALSE}
# Show the first rows of the table corresponding to one sample, from emulated data.
head(sim_boot_data()[[2]][[1]])
```

* In the case of option 3, each column represents a sample. Therefore, each condition is represented by a single table containing 
the relevant samples.
* In the case of option 2, each column represents a bootstrap iteration and each table represents a sample. Therefore, each 
condition is represented by a list of tables.

### Read counts, TPMs, etc

RATs will happily crunch any type of numeric value, but the type of abundances you provide will have an impact on the DTU outcome. The statistical test
employed (G test) is meant to work with actual counts, not arbitrarily scaled values like TPMs (transcripts per million reads) or other per-million-reads representations.
On the other hand, read counts work well with RATs and achieve a good False Discovery Rate, but they are influenced by 
the length of the transcripts and a well-expressed long isoform can drown out a valid change in the relative abundances of short isoforms.

To get the best of both worlds, we recommend obtaining TPM abundances, so as to account for transcript lengths, and then scaling these values to your actual library size
to cancel out the per-million part and regain realistic counts. RATs provides functionality to enable you to do this, the first of which is the `scaleto` parameter in 
`fish4rodents()`, shown above.


## Annotation

Regardless of data format, RATs also needs an annotation `data.frame` or `data.table` that matches transcript identifiers to gene identifiers. 
By default the column labels are `target_id` for the transcript IDs and `parent_id` for the gene IDs. These label values can be overridden (see Additional Settings later in this vignette). The annotation table looks like this:

```{r, echo=FALSE}
# Show the first rows of the table corresponding to the annotation, from simulated data.
head(sim_count_data()[[1]])
```

Such a table can be compiled with a variety of methods and from a variety of resources. RATs provides functionality
to create this table from a GTF file or a GRanges object with GTF-style metadata columns. 
(**Note:** GFF3 is not supported for this)

```{r, eval=FALSE}
# Extract transcript ID to gene ID index from a GTF annotation.
myannot <- gtf2ids("my_annotation_file.gtf")
# !! gtf2ids() was previosuly called annot2ids(). The old name is still available but will eventally be discontinued.

# Extract transcript ID and gene ID from a GRanges object. It must have GTF-style metadata columns "gene_id" and "transcript_id".
myannot <- granges2ids(mygranges)
```

Extracting the ID pairs from a TxDb is simpler and does not require a dedicated helper function:

```{r eval=FALSE}
myannot <- select(mytxdb, keys(mytxdb), "TXNAME", "GENEID")
# Rename the columns to match what RATs expects, or remember to tell call_DTU() what the column names are.
names(myannot) <- c('gene_id', 'target_id')
```


***


# Calling DTU 

To bypass the complexity of involving third-party tools in this tutorial, we will instead use **emulated data**. 
RATs comes with data emulators, intended to be used for testing the code. However, they are also convenient to 
use for showcasing how RATs works.

By default, RATs reports on its progress and produces a summary report. These can be suppressed using the 
`verbose = FALSE` parameter. This also suppresses some warnings generated by RATs when checking the sanity of its input, 
so we do not recommend it for general use. However, to prevent cluttering this tutorial with verbose output, we will use this option in our examples. 

If you choose to allow verbose output when trying out the examples below using the emulated data, you will get some **warnings**
about the number of bootstraps. The warning is triggered because the emulated dataset used in the examples immitates only the structure of 
real data, not the actual volume of it, and as such it contains unrealistically few bootstrap iterations. Simply ignore these warnings
for these examples.


### DTU from abundance estimates, without bootstraps

This is the simplest usage case, and your likely input type if you use quantifications methods other than Kallisto and Salmon.

First, let's emulate some data to work with:

```{r, eval=FALSE}
# Simulate some data.
simdat <- sim_count_data(clean=TRUE)
# For convenience let's assign the contents of the list to separate variables
mycond_A <- simdat[[2]]       # Simulated abundances for one condition.
mycond_B <- simdat[[3]]       # Simulated abundances for other condition.
myannot <- simdat[[1]]        # Transcript and gene IDs for the above data.
```

Now we can call DTU:

```{r, eval=FALSE}
# Find DTU between the simulated datasets.
mydtu <- call_DTU(annot= myannot, count_data_A= mycond_A, 
                  count_data_B= mycond_B, verbose= FALSE,
                  name_A= "healthy", name_B= "patients", 
                  varname= "My phenotype",
                  description="Comparison of two simulated counts datasets 
                  for the tutorial. Simulated using built-in functionality 
                  of RATs.")
```

#### Mandatory arguments:

1. `annot` - An annotation data frame, as described in the *Input formats* section.
2. `count_data_A` and `count_data_B` - are each a `data.table` of transcript abundances as described in the Input section.

#### Optional arguments (will be recorded in the output object):

* `name_A`, `name_B` - A name for each conditon. (Default `NA`)
* `varname` - The name of the variable/condition. (Default `NA`)
* `description` - Free-text description. (Default `NA`)

#### Advanced options

Refer to the respective sections later in this vignette.


### DTU from bootstrapped abundance estimates

First, let's emulate some data, as we did before.

```{r}
# Simulate some data. (Notice it is a different function than before.)
simdat <- sim_boot_data(clean=TRUE)

# For convenience let's assign the contents of the list to separate variables
mycond_A <- simdat[[2]]   # Simulated bootstrapped data for one condition.
mycond_B <- simdat[[3]]   # Simulated bootstrapped data for other condition.
myannot <- simdat[[1]]    # Transcript and gene IDs for the above data.
```

Now we can call DTU:

```{r, eval=FALSE}
# Find DTU between conditions.
mydtu <- call_DTU(annot= myannot, boot_data_A= mycond_A, 
                  boot_data_B= mycond_B, verbose= FALSE, 
                  name_A= "wildtype", name_B= "some mutant", 
                  varname = "My phenotype", description="Comparison of 
                  two simulated datasets of bootstrapped counts for the 
                  tutorial. Simulated using built-in functionality 
                  of RATs.")
```

#### Mandatory arguments:

1. `annot` - An annotation data frame, as described in the *Input formats* section.
2. `boot_data_A` and `boot_data_B` - are each a list of `data.table` objects, as described in the Input section.

#### Optional arguments (will be recorded in the output object):

* `name_A`, `name_B` - A name for each conditon. (Default `NA`)
* `varname` - The name of the variable/condition. (Default `NA`)
* `description` - Free-text description. (Default `NA`)

#### Advanced options

Refer to the respective sections later in this vignette.


### DTU with Salmon/Kallisto output

RATs offers a method to import bootstrapped abundances directly from [Salmon](https://combine-lab.github.io/salmon/) 
output (requires [wasabi](https://github.com/COMBINE-lab/wasabi)) or  [Kallisto](https://pachterlab.github.io/kallisto/) 
output. The raw abundances are normalised to TPM (default).

```{r, eval=FALSE}
# 1. Collect your outputs into vectors. The end of each path should be a 
#    directory with a unique name/identifier for one sample.
samples_A <- c("your/path/SAMPLE1", "your/path/SAMPLE4","your/path/SAMPLE5")
samples_B <- c("your/path/SAMPLE2", "your/path/SAMPLE3","your/path/SAMPLE7", 
               "your/path/SAMPLE10")

# 2. Calculate length- & library-normalised abundances. 
#    Scale them to 1M reads for TPM values.
mydata <- fish4rodents(A_paths= samples_A, B_paths= samples_B, 
                       annot= myannot, scaleto=100000000)
```

The return value of `fish4rodents()` is a list with two items: `$boot_data_A` and `$boot_data_B`. These two items are
formatted for RATs' generic bootstrapped input. This helper function works *only* with bootstrapped quantifications.
The abundaces are normalised for isoform length and library-size.
Scaling to 1M reads provides TPMs, which can be used by other tools as well. Other scaling options are discussed in a
dedicated section later in this vignette.

#### Mandatory arguments (`fish4rodents`):

1. `A_paths` and `B_paths` - Two vectors containing the paths to the quantification output directories, one vector for each condition. The
last segments of each path should be a directory with a unique identifying name for a single sample.
2. `annot` - The annotation table that you will use for calling DTU (and that was used for the quantification).
This is needed to enforce consistent order of the transcripts in the tables, enabling efficient processing.

#### Optional arguments (`fish4rodents`):

* `scaleto` allows you to control the normalisation factor. (Default 1000000 gives TPM values).
* `half_cooked` indicates whether a kallisto-style abundance.h5 file already exists. (Default `FALSE`). `wasabi` automatically detects the presence of an abundance.h5 file, so this option is actually redundant and pointless.
* `beartext` directs `fish4rodents()` to read bootstrap data from plain-text files in a `bootstraps` subdirectory in each sample instead of parsing the abundance.h5 file of the sample. `kallisto` has the option to return plaintext results or to extract results from an existing abundance.h5 file to plaintext using its `h5dump` subcommand. (Default FALSE)

And finally run DTU in the way already shown:

```{r, eval=FALSE}
# 3. Run RATs with the bootstrapped table data format. 
#    Scale the TPM abundances to the respective library sizes.
#    Scaling factors are: (target size) / (current size)
#    Here the current size is 1M (ie. TPMs), as per the fish4rodents() step.
#    For the example, assume library sizes of 25M, 26M, 23M, 50M, 45M, 
#    48M and 52M for the 7 samples, in the same order.
mydtu <- call_DTU(annot= myannot, boot_data_A= mydata$boot_data_A, 
                  boot_data_B= mydata$boot_data_B, verbose= FALSE, 
                  scaling=c(25, 26, 23, 50, 45, 48, 52))
```

You will notice that here there is another scaling step. RATs' tests require count-like abundance values, which TPMs are not.
Providing a vector of scaling factors (one per sample, in order starting from condition A and finishing with condition B),
allows the TPMs to be scaled up to the real library sizes, while maintaining the length-normalisation of TPMs. Scaling
is discussed in its own section later in this vignette.


***


# Advanced Parameters and Settings


In summary, `rats` works as follows:

![RATs design](./figs/rats_workflow.jpg)


## Thresholds

The following three main thresholds are used in RATs:

```{r, eval=FALSE}
# Calling DTU with custom thresholds.
mydtu <- call_DTU(annot= myannot, 
                  boot_data_A= mycond_A, boot_data_B= mycond_B,
                  p_thresh= 0.01, dprop_thres = 0.15, abund_thresh= 10)
```

1. `p_thresh` - Statistical significance level. P-values below this will be considered significant. Lower threshold values are stricter, and help reduce low-count low-confidence calls. (Default 0.05, very permissive)
2. `dprop_thresh` - Effect size threshold. Transcripts whose proportion changes between conditions by less than this threshold are considered uninteresting, regardless of their statistical significance. (Default 0.20, quite strict)
3. `abund_thresh` - Noise threshold. Minimum mean (across replicates) abundance for a transcript to be considered expressed. Transcripts with mean abundances below this will be ignored. Mean total gene count must also meet this value in both conditions, a.k.a. at least one expressed isoform must exist in each condition. (Default 5, very permissive)

The default values for these thresholds have been chosen such that they achieve a *median* FDR <5% for a high quality dataset from *Arabidopsis thaliana*, even with only 3 replicates per condition. 
Your mileage may vary and you should give some consideration to selecting appropriate values. 

Depending on the settings, *additional thresholds* are available and will be discussed in their respective sections below.


## Bootstrapping 

RATs offers two types of bootstrapping:

1. Bootstrapping of significance and effect size against the technical fluctuations of the quantifications. This requires bootstrapped quantifications as input.
2. Bootstrapping of significance and effect size against the abundance fluctuations among samples.

Enabling these two procedures assesses the robustness of the DTU calls. Calls that are easily overturned depending on which subset of the data is used
may be spurious but may also be valid. Deeper sequencing and/or higher replication may resolve these ambiguities.

In both cases, what is measured is the fraction of iterations in which the significance and effect size meet their respective thresholds. 
These fractions can be used as indicators of confidence in the calls, especially when a large number of iterations is performed.

If bootstrapping is switched off, the respective fields will not be included in the output.


### Quantification bootstraping

Three parameters control bootstrapping of DTU calls against the quantification uncertainty:

1. `qboot` - Whether to bootstrap the quantifications or not. (Default TRUE)
2. `qbootnum` - How many bootstrap iterations to do. Preferably at least 100. (Default 0) If 0, 
RATs will try to infer a value from the data (currently equal to the number of bootstraps in the data).
3. `qrep_thresh` - Reproducibility threshold. What fraction of the iterations has to agree on a result to consider it confident. (Default 0.95)
If you want to calculate the reproducibility but don't want it to weigh in on the DTU classifications, set the threshold to 0.

In this process, one quantification iteration will be randomly selected from each sample and DTU will be called on it. This will be repeated `qbootnum` times. Because the
number of replicates remains the same, the statistical power is not compromised. Therefore, the reproducibility will be **used as a criterion** in calling DTU, along with
statistical significance and effect size. The process is **stochastic**; the quantifications are randomly sampled, so runs on the same data may yield slight differences in DTU. 
Using higher `qbootnum` improves reproducibility between runs on the same dataset.

Low reproducibility indicates that the quantification tool found it hard to distinguish these transcripts. This can be caused by high similarity of the isoforms, genes with a large
number of isoforms, and/or poor read coverage in the regions differentiating the isoforms from one another. In these cases, skepticism is required about the quantifications and any 
potential differential expression regarding these transcripts.

Warnings will be generated if `qbootnum` is too low or too high, but in most cases RATs will continue with the analysis. The warnings will not be shown if `verbose = FALSE`.

```{r, eval=FALSE}
# Bootstrap (default). Do 100 iterations.
mydtu <- call_DTU(annot = myannot, 
                  boot_data_A= mycond_A, boot_data_B= mycond_B,
                  qboot = TRUE, qbootnum = 100, qrep_thresh= 0.95)

# Skip bootstraps.
mydtu <- call_DTU(annot = myannot, 
                  boot_data_A= mycond_A, boot_data_B= mycond_B,
                  qboot = FALSE)
```


### Replicate bootstraping

Three parameters control bootstrapping of DTU calls agaisnt the samples:

1. `rboot` - Whether to bootstrap the replicates or not. (Default TRUE)
2. `rrep_thresh` - Reproducibility threshold. What fraction of the iterations has to agree on a result to consider it confident. (Default 0.85)
If you want to calculate the reproducibility but don't want it to weigh in on the DTU classifications, set the threshold to 0.

Unlike bootstrapping the quantifications, bootstrapping the replicates is currently **not stochastic**. RATs will do ALL the 1 vs 1 combinations of samples, one from each condition.
This behaviour may be changed if in the future there is demand for high replication that reaches the capacity limits of R matrices.
To maintain comparable statistical significance to the actual experimental design, because only one replicate is used each time, the abundances are scaled up by the number of replicates in the condition. This is equivalent to the idealised scenario of all the replicates being extremely consistent and similar to the chosen one.

```{r, eval=FALSE}
# Bootstrap (default).
mydtu <- call_DTU(annot = myannot, 
                  boot_data_A= mycond_A, boot_data_B= mycond_B,
                  rboot = TRUE, qrep_thresh= 0.85)

# Skip bootstraps.
mydtu <- call_DTU(annot = myannot, 
                  boot_data_A= mycond_A, boot_data_B= mycond_B,
                  rboot = FALSE)
```

**Note** that for few replicates, the reproducibility values are highly discrete:
The number of iterations for 3 samples per condition is `3 * 3 = 9`. 
So the minimum possible error rate is `1 / 9 = 0.111111...`. 
The corresponding threshold is `1-0.1111111... = 0.8888888...`, hence the default threshold of 0.85.

### Extra bootstrapping info

Additional insight on the variance across bootstraps can be obtained if desired. 
Up to RATs version `0.6.4` this information was alsways included. Calculating this information requires several columns each with length equal to the number of genes for each bootstrap iteration to be kept in memory. This caused a considerable memory footprint for bootstrapped runs and posed a physical constraint in scalability. `lean` mode removes this contraint, and results in the memory needed by RATs remaining low and constant regardless of number of iterations. 

1. `lean` - Default TRUE will only report the frequency of DTU call per gene in the iterations and whether that frequency passes the threshold. FALSE will additionally report the median/mean/max/min values of `Dprop` and `pval_corr` per gene across iterations, and the frequency with which the gene was not eligible for testing due to low expression in the selected subset. 

```{r, eval=FALSE}
# Lean run (default).
mydtu <- call_DTU(annot = myannot, 
                  boot_data_A= mycond_A, boot_data_B= mycond_B,
                  lean =TRUE)

# Extra info on variance across iterations.
mydtu <- call_DTU(annot = myannot, 
                  boot_data_A= mycond_A, boot_data_B= mycond_B,
                  lean = FALSE)
```


## Multi-threading

RATs completion time depends on the number of annotated and expressed transcripts. Single-threaded, RATs can take up to a few minutes per iteration for large annotations.
Fortunately, the task is highly parallelisable:

```{r eval=FALSE}
# Using 8 threads/cores for parallel computing.
mydtu <- call_DTU(annot = myannot, 
                  boot_data_A= mycond_A, boot_data_B= mycond_B,
                  threads = 8)
```

1. `threads` - The number of threads to use. (Default 1)

Due to core R implementation limitations, the type of multi-threading used in RATs works only in POSIX-compliant systems (Linux, Mac, not Windows).
Refer to the `parallel` package for details.


## Test selection

RATs runs both gene-level DTU calls and transcript-level DTU calls. They are independent from one another and we consider them complementary and recommend using them together, but the option to skip either is provided for special use cases. 
The output fields of the skipped test will be filled with `NA`.

```{r, eval=FALSE}
# Transcripts only.
mydtu <- call_DTU(annot = myannot, 
                  boot_data_A= mycond_A, boot_data_B= mycond_B,
                  testmode="transc")
# Genes only.
mydtu <- call_DTU(annot = myannot, 
                  boot_data_A= mycond_A, boot_data_B= mycond_B,
                  testmode="genes")
```

1. `testmode` - Which test(s) to run {"transc", "genes", "both"}. (Default "both")

## Correction for multiple testing

Testing multiple null hypotheses increases the chance of one being falsely rejected. To keep the overall false rate at the 
desired level, the raw p-values must be adjusted. The default adjustment method is `BH` (Benjamini-Hochberg). A full list of 
options is listed in R's `p.adjust.methods`.

```{r, eval=FALSE}
# Bonferroni correction.
mydtu <- call_DTU(annot = myannot, 
                  boot_data_A= mycond_A, boot_data_B= mycond_B,
                  correction = "bonferroni")
```

1. `correction` - Type of multiple testing correction. (Default "BH")

## Input field names

RATs needs to pull information from different fields of the data and annotation and it does so based on the names of the columns. 
You can override the default names of these fields.

### Annotation field names

Although it is easy to rename the columns of a table to comply with the expected names, this may sometimes be undesireable, so RATs
allows you to change the expected names instead.

```{r, eval=FALSE}
# Call DTU using annotation with custom field names.
mydtu <- call_DTU(annot = myannot, 
                  boot_data_A= mycond_A, boot_data_B= mycond_B,
                  TARGET_COL="transcript", PARENT_COL="gene")
```

1. `TARGET_COL` - The name of the field holding the transcript identifiers in the annotation data frame. (Default "target_id")
2. `PARENT_COL` - The name of the field holding the respective gene identifiers in the annotation data frame. (Default "parent_id")

The `TARGET_COL` and `PARENT_COL` parameters are also available for `denest_sleuth_boots()` and `fish4rodents()`.


## Abundance scaling

As mentioned previously, various commonly-used normalised abundance units (like TPM) are scaled to an arbitrary and usually 
smaller-than-actual sample size (often 1 million reads or fragments). This artificially deflates the significances from count-based tests, 
such as those employed by RATs, and reduces the statistical power of the method. 

To counter this, RATs provides the option to scale abundaces either equally by a single factor (such as average library size among samples) 
or by a vector of factors (one per sample). The former maintains any pre-existing library-size normalisation among samples. This is necessary
for fold-change based methods, but RATs does not require it. Instead, using the respective actual library sizes of the samples allows the
higher-throughput samples to have a bigger influence than the lower-throughput samples. This is particularly relevant if your samples have very dissimilar 
library sizes.

For flexibility with different types of input, these scaling options can be applied in either of two stages: The data import step by `fish4rodents()`, 
or the actual testing step by `call_DTU()`. In the example examined previously, `fish4rodents()` was instructed to create TPM abundances, 
by normalising to `1000000` reads. Such values are useful with certain other tools that a user may also intend to use.
Subsequently, these TPMs were re-scaled to meet the library size of each sample, thus providing RATs with count-like abundance values that retain
the normalisation by isoform length. However, it is not necessary to scale in two separate steps. 

Both `fish4rodents()` and `call_DTU()` support scaling by a single value or a vector of values. If you don't need the TPMs, you can scale 
directly to the desired library size(s), as in the examples below:

```{r, eval=FALSE}
# The following are equivalent.

# 1:
# Scale directly to library sizes at the import step.
mydata <- fish4rodents(A_paths= samples_A, B_paths= samples_B, 
                       annot= myannot, 
                       scaleto=c(25123456, 2665431, 23131313, 
                                 5000000, 45123132, 48456654, 52363636))
# No additional scaling needed.
mydtu <- call_DTU(annot= myannot, boot_data_A= mydata$boot_data_A, 
                  boot_data_B= mydata$boot_data_B, 
                  scaling=1)  # default

# 2:
# Normalise quantifications but do not scale them.
mydata <- fish4rodents(A_paths= samples_A, B_paths= samples_B, 
                       annot= myannot, 
                       scaleto=1)
# Scale directly to the smallest library size at the run step.
libsiz <- min(25123456, 2665431, 23131313, 5000000, 45123132, 48456654, 52363636)
mydtu <- call_DTU(annot= myannot, boot_data_A= mydata$boot_data_A, 
                  boot_data_B= mydata$boot_data_B,  
                  scaling=libsiz)

# 3:
# Scale Kallisto/Salmon quantifications to TPMs.
mydata <- fish4rodents(A_paths= samples_A, B_paths= samples_B, 
                       annot= myannot, 
                       scaleto=10000000)  # default

# Scale TPMs to actual library sizes.
mydtu <- call_DTU(annot= myannot, boot_data_A= mydata$boot_data_A, 
                  boot_data_B= mydata$boot_data_B,
                  scaling=c(25.123456, 26.65431, 23.131313, 50.0, 45.123132, 48.456654, 52.363636))
```

You can mix and match scaling options as per your needs, so take care to ensure that the scaling you apply is appropriate.
*It is important to note*, that if you simply run both methods with their respective defaults, you'll effectively run RATs 
on TPM values, which is extremely underpowered and not recommended. Please, provide appropriate scaling factors for your data.


## Annotation discrepancies

Different annotation versions often preserve transcript IDs, despite altering the details of the transcript model. 
They also tend to include more or fewer transcripts, affecting the result of quantification.
It is important to use the same annotation throughout the workflow, otherwise the abundances will not be comparable
in a meaningful way.

RATs will abort the run if the set of feature IDs in the provided annotation does not match fully the set of IDs in the quantifications.
If this happens, ensure you are using the exact same annotation throughout your workflow.

For special use cases, RATs provides the option to ignore the discrepancy and pretend everything is OK. Do this at your own risk.

```{r, eval=FALSE}
mydtu <- call_DTU(annot= myannot, boot_data_A= mydata$boot_data_A, 
                  boot_data_B= mydata$boot_data_B,
                  reckless=TRUE, verbose=TRUE)
```


In reckless mode, all internal operations and the output of RATs are based on the annotation provided:

* Any transcript IDs present in the data but missing from the annotation will be ignored and will not show up in the output at all, as they cannot be matched to the gene IDs.
* Any transcript ID present in the annotation but missing from the data will be included in the output as zero expression. They can be identified later as `NA` values in `$Transcripts[, .(stdevA, stdevB)]` as these fields are not used downstream by RATs. `NA` values in other numeric fields are explicitly overwritten with `0` to allow downstream interoperability.

***


# Contact information

The `rats` R package was developed within [The Barton Group](http://www.compbio.dundee.ac.uk) at [The University of Dundee](http://www.dundee.ac.uk)
by Dr. Kimon Froussios, Dr. Kira MourÃ£o and Dr. Nick Schurch.

To **report problems** or **ask for assistance**, please raise a new issue [on the project's support forum](https://github.com/bartongroup/Rats/issues).
Providing a *reproducible working example* that demonstrates your issue is strongly encouraged to help us understand the problem. Also, be sure 
to **read the vignette(s)**, and browse/search the support forum before posting a new issue, in case your question is already answered there.

Enjoy!

![](./figs/rats_logo.png)
